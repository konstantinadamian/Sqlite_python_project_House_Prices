# -*- coding: utf-8 -*-
"""_sql_tech_pro_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NrIxy-EVxTVJw9kOFmIF5juuq_MqggdV
"""

#import libraries
import sqlite3
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.simplefilter('ignore')
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score ,mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

#Database connection
connect_ = sqlite3.connect('/content/TechPro-DataScience-Midseason-Dataset.db')

#I retrieve the names of all tables from the database and print them
cursor = connect_.cursor()

cursor.execute('SELECT name FROM sqlite_master WHERE type="table";')
tables = cursor.fetchall()
print('My table : ')
for t in tables:
    print(t[0])

#For each table, we display its columns and data types.
tables = ['houses', 'cities', 'energy_classes']

for t in tables:
    print(f'\n My table with column is: {t}')
    cursor.execute(f'PRAGMA table_info({t});')
    for c in cursor.fetchall():
      #I Print the name and data type of each column
        print(f' - {c[1]} ({c[2]})')

#Sql query to retrieve data from the tables houses, cities, energy_classes

from sqlite3.dbapi2 import connect

my_query_ = """
SELECT
    h.Area,
    h.Bedrooms,
    h.Bathrooms,
    h.'Energy Class',
    h.'Year Built',
    h.Price,
    h.'City',
    c.Crime,
    e.'Tax Rate'
    FROM houses as h
JOIN cities as c ON h.City = c.'index'
JOIN energy_classes as e ON h.'Energy Class' = e.'index';
"""
#Execute the query in the database

cursor.execute(my_query_)

#Retrieve all results
rows = cursor.fetchall()

#List of column names from the query result
columns = [i[0] for i in cursor.description]

#I Create DataFrame from the results
data = pd.DataFrame(rows, columns=columns)

#Close the database connection
connect_.close()

#Print the first five rows
print(data.head())

#Check how many duplicate have in my dataset
data.duplicated().sum()

#Revome duplicate
data = data.drop_duplicates()

#Check how many missing values exist in dataset
data.isna().sum()

#Check unique values in column 'City'
data['City'].unique()

#One-hot encoding for the 'City' column, removing the first category
data_dummies = pd.get_dummies(data['City'], drop_first=True).astype(int)

#Remove column 'City'
data = pd.concat([data.drop('City', axis=1), data_dummies], axis=1)

data.head()

#Check unique values for the column 'Energy Class'
data['Energy Class'].unique()

#Convert to a categorical variable, from which energy class is better quality to those with the lowest quality
energy_ = {
    'A': 5,
    'B': 4,
    'C': 3,
    'D': 2,
    'E': 1
}

data['Energy Class'] = data['Energy Class'].map(energy_)

#chech my dataset again
data.head()

#I choose a histogram for graph to see the distribution
data['Price'].hist()

#I choose boxplot for graphs to see some statistics of the column that I will use for prediction.
sns.boxplot(data=data['Price'])

#Because I have negative values ​​in the price of houses, I make a mask where I will have the data filtered to be above 0.
data = data[data['Price'] > 0]

#statistics
data.describe()

#I'm creating a graph to see how much the cost of a house is affected by crime
sns.barplot(data=data,x='Crime',y='Price')
plt.xticks(rotation=45)
plt.tight_layout()

#I remove all nan from the prediction column
data = data.dropna(subset=['Price'])

#I put the median in the nan of the columns
for i in ['Area', 'Bedrooms', 'Bathrooms']:
  data.fillna(data[i].median(), inplace=True)

#chech information again
data.info()

#check unique values
data['Year Built'].unique()

#convert from str nan to regular nan
data['Year Built'] = data['Year Built'].replace('nan', np.nan)

#change type
data['Year Built'] = data['Year Built'].astype(float)

#change type again
data['Year Built'] = data['Year Built'].astype('Int64')

#chech nan
data['Year Built'].isna().sum()

#revome nan
data = data.dropna(subset=['Year Built'])

#chech correlation columns
correlation_column = data.corr()
correlation_column

#graph for i see correlation
sns.heatmap(correlation_column, annot=True,cmap='coolwarm', fmt=".1f")
plt.show()

x_ = data.drop(['Price'],axis=1)
y_ = data['Price']

#split model train/test
x_train, x_test, y_train, y_test = train_test_split(x_, y_, test_size=0.2, random_state=80)

# z-scores data
scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

#Create linearegression model
model_data = LinearRegression()
model_data.fit(x_train_scaled, y_train)

#predict
pred_test = model_data.predict(x_test_scaled)
pred_test

#Calculating R2
r_squared = r2_score(y_test, pred_test)
'R2_score test : {0:.2f}'.format(r_squared)

#Root Mean Squared Error
rmse_ = np.sqrt(mean_squared_error(y_test, pred_test))
print('RMSE test:', rmse_)

#predict train
pred_train = model_data.predict(x_train_scaled)
pred_train

#Calculating R2 train
r_squared_train = r2_score(y_train, pred_train)
'R2_score train: {0:.2f}'.format(r_squared_train)

#Root Mean Squared Error for train
rmse_train = np.sqrt(mean_squared_error(y_train, pred_train))
print('RMSE train:', rmse_train)

#graphs to see how well my model predicts, how well the line fits the data
plt.scatter(y_test, pred_test, color='cornflowerblue')
plt.plot(y_test, y_test, color='coral', linewidth=2)
plt.xlabel('actual prices')
plt.ylabel('predicted prices')
plt.title('actual / predicted prices')
plt.grid(True,color='black')
plt.show()

#the errors of my model
residuals = y_test - pred_test
plt.figure(figsize=(7, 4))
plt.scatter(pred_test, residuals, alpha=0.6)
plt.axhline(0, color='red', linestyle='--')
plt.grid(True)
plt.show()

from sklearn.model_selection import cross_val_score, KFold

#created by KFold
kf = KFold(n_splits=10, shuffle=True, random_state=80)

#result
neg_mse_scores = cross_val_score(model_data, x_, y_, scoring='neg_mean_squared_error', cv=kf)
rmse_scores = np.sqrt(-neg_mse_scores)

print('RMSE per fold:', rmse_scores)
print('Mean RMSE:', rmse_scores.mean())

from sklearn.tree import DecisionTreeRegressor

#Create decisiontree model
decisiontree_ = DecisionTreeRegressor(max_depth=6, min_samples_split=8, random_state=60)
decisiontree_.fit(x_train, y_train)

#prediction
y_pred_decisiontree = decisiontree_.predict(x_test)

r_squared_dt = r2_score(y_test, y_pred_decisiontree)
'R2_score test decision TreeRegressior: {0:.2f}'.format(r_squared_dt)

rmse_tree = np.sqrt(mean_squared_error(y_test, y_pred_decisiontree))
print('Decision Tree Test RMSE:', rmse_tree)

y_train_decisiontree = decisiontree_.predict(x_train)

r_squared_dt_2 = r2_score(y_train, y_train_decisiontree)
'R2_score test decision TreeRegressior: {0:.2f}'.format(r_squared_dt_2)

rmse_train_tree = np.sqrt(mean_squared_error(y_train, y_train_decisiontree))
print('Decision Tree Train RMSE:', rmse_train_tree)

labels = ['Train RMSE', 'Test RMSE']
rmse_values = [rmse_train_tree, rmse_tree]
#graphs to check if there is overfitting
plt.figure(figsize=(6, 4))
plt.bar(labels, rmse_values)
plt.ylabel('RMSE')
plt.title('Decision Tree Regression RMSE (Train vs Test)')
plt.grid(True, axis='y')
plt.show()

labels = ['Linear Train', 'Linear Test', 'Tree Train', 'Tree Test']
rmse_values = [rmse_train, rmse_, rmse_train_tree, rmse_tree]

#I check which model has better error performance, and if it differs from the test to the train
plt.figure(figsize=(8, 5))
plt.bar(labels, rmse_values)
plt.ylabel('RMSE')
plt.title('compare RMSE: Linear Regression vs Decision Tree')
plt.grid(axis='y')
plt.show()